# MyOwnGradProjcet
Project Title: An app to aid blind people in Kitchen.
We want to assist blind individuals use smartphones during a helpful approach during this project, our major domain here to create them communicate with mobile devices and perceive things around them exploitation their voices. Blind individuals face several difficulties whereas exploitation smartphones, adore creating calls, reading messages, exploitation package and recognizing things around them. Recognition of daily living by computer-based act recently gained an excellent deal of interest thanks to its relevancy to close assisted living. Such applications need the automated recognition of high-level activities composed of multiple actions disbursed during a given setting by relatives. we tend to deliver a deep neural design for the room activity recognition, exploitation the machine learning models and handmade options ensemble to extract additional information data. Experiments show that our technique is at the innovative to classify cookery behavior during a well-known room dataset. during this App we tend to aim to specifically determine and facilitate blind individuals to cook room things. during this paper, we tend to address the matter of serving to humans cook in an enclosed mobile camera that identifies cookery tools and food ingredients. Our approach is targeted on supporting individuals once they're within the room, with the last word goal of understanding cookery instrumentation and food ingredients. Our methodology is predicated on a deep neural design comprising multiple convolutionary neural networks that are amalgamate before the action classification is performed. we tend to perform experiments exploitation the room Scene Context based mostly Gesture Recognition dataset (KSCGR) (Shimada et al. 2013), and that we show that our planned approach outperforms the present state-of - the art technique (Bansal et al. 2013) for this explicit dataset.
